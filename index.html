<!DOCTYPE html>
<html>
<head>
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-168011222-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-168011222-1');
	</script>
	<!-- Google fonts -->
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700">
	<!-- Site info -->
	<title>Tyler A. Chang</title>
	<meta name="author" content="Tyler A. Chang">
	<meta name="description" content="Tyler Chang, PhD student at UC San Diego.">
	<meta name="keywords" content="Tyler Chang, Carleton, UC San Diego, UCSD">
	<meta name="google-site-verification" content="rCmbByGQ7v60goauxCsOqJSLGF2JA8BB2EkoA2E05r8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<!--Open Graph tags -->
	<meta property="og:title" content="Tyler A. Chang" />
	<meta property="og:type" content="website" />
	<meta property="og:url" content="https://tylerachang.github.io/" />
	<meta property="og:image" content="hawaii.jpg" />
	<meta property="og:image:width" content="3264" />
	<meta property="og:image:height" content="2448" />
	<meta property="og:description" content="Tyler Chang, PhD student at UC San Diego." />
	<!-- Links -->
	<link rel="canonical" href="https://tylerachang.github.io/">
    <link rel="shortcut icon" href="icon.ico">
    <link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>
    <div id="sidebar"></div>
	<div id="topbar">
	</div>
	<div id="main-box">
		<div class="content">
			<div class="header">Tyler Chang</div>
            <div class="subheader">PhD Student at UC San Diego.</div>
            <!-- &nbsp<a class="paragraph" style="display:inline;font-weight:bold;" target="_blank" href="tyler_chang_cv.pdf">[Link to CV]</a> -->
			<!-- <div class="paragraph" style="font-weight:bold;">Language Models / Pretraining Dynamics / Multilinguality.</div><br> -->
			<div class="paragraph"><b>Hello!</b>
			I am a cognitive science PhD student at UC San Diego working on the analysis of large language models, particularly during pretraining. 
			My research aims to enable more transparent, inclusive, and auditable language technologies.
			My CV is <a target="_blank" href="tyler_chang_cv.pdf">[here]</a>.
			</div><br>
			<img id="selfimage" src="sealions.jpg" alt="Sea lions in La Jolla Cove.">
			<div class="paragraph">
			Previously, I've spent time working at Google DeepMind, Google Research, and Amazon Science.
			I completed my undergrad at Carleton College in Northfield, Minnesota, majoring in math and cognitive science.
			Outside of research, I enjoy playing piano, running, and taking blurry photos in the ocean. For questions about my research, contact me at <span>tachang@ucsd.edu</span>!
			</div><br>
			
			<hr><hr>
			<div class="subheader" style="display:inline;">Recent Highlights</div>
			<ul>
				<li>We published a <a target="_blank" href="https://medium.com/people-ai-research/scaling-training-data-attribution-f7d1eddd85da">[blog post]</a>, <a target="_blank" href="https://arxiv.org/abs/2410.17413">[preprint]</a>, and <a target="_blank" href="https://github.com/PAIR-code/pretraining-tda/">[demo]</a> for our work at Google DeepMind scaling training data attribution methods to LLM pretraining!</li><br>
				<li>Our <a target="_blank" href="https://arxiv.org/abs/2311.09205">[paper]</a> on the curse of multilinguality in language models received an outstanding paper award at EMNLP 2024!</li><br>
				<li>We released <a target="_blank" href="https://github.com/tylerachang/goldfish">[Goldfish]</a>, a suite of small, comparable monolingual language models for 350 languages!</li>
			</ul>
			<br>
			
			<hr><hr>
			<div class="subheader" style="display:inline;">Select Publications</div>&nbsp&nbsp
			<a class="paragraph" style="display:inline;" target="_blank" href="https://scholar.google.com/citations?user=zkDuqfwAAAAJ">[Google Scholar]</a>&nbsp
			<!-- <a class="paragraph" style="display:inline;" target="_blank" href="https://www.semanticscholar.org/author/Tyler-A.-Chang/2087001989">[Semantic Scholar]</a>&nbsp -->
			<a class="paragraph" style="display:inline;" target="_blank" href="https://github.com/tylerachang">[Github]</a>&nbsp
			<!-- <a class="paragraph" style="display:inline;" target="_blank" href="tyler_chang_cv.pdf">[CV]</a> -->
			<br><br>
			
			<details>
				<summary style="cursor:pointer">
				<b>&nbsp Pretraining dynamics and language acquisition</b>
				</summary>
				<img id="selfimage" src="research_images/learning_curve.png" alt="Language model learning curve">
				<div class="publication"><b>Chang, T. A.</b>, Rajagopal, D., Bolukbasi, T., Dixon, L., & Tenney, I. (under review). Scalable influence and fact tracing for large language model pretraining.&nbsp <a target="_blank" href="https://arxiv.org/abs/2410.17413">[Preprint]</a>&nbsp <a target="_blank" href="https://medium.com/people-ai-research/scaling-training-data-attribution-f7d1eddd85da">[Blog Post]</a>&nbsp <a target="_blank" href="https://github.com/PAIR-code/pretraining-tda/">[Demo]</a></div>
				<div class="publication"><b>Chang, T. A.</b>, Tu, Z., & Bergen, B. K. (2024). Characterizing learning curves during language model pre-training: Learning, forgetting, and stability. <i>Transactions of the Association for Computational Linguistics</i> (TACL).&nbsp <a target="_blank" href="https://arxiv.org/abs/2308.15419">[Preprint]</a></div>
				<div class="publication">Unger, L., <b>Chang, T. A.</b>, Savic, O., Bergen, B. K., & Sloutsky, V. M. (2024). When is a word in good company for learning? <i>Developmental Science</i>.&nbsp <a target="_blank" href="https://onlinelibrary.wiley.com/doi/10.1111/desc.13510">[Paper]</a></div>
				<div class="publication"><b>Chang, T. A.</b>, & Bergen, B. K. (2022). Word acquisition in neural language models. <i>Transactions of the Association for Computational Linguistics</i> (TACL). Presented at ACL 2022.&nbsp <a target="_blank" href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00444/109271/Word-Acquisition-in-Neural-Language-Models">[Paper]</a>&nbsp <a target="_blank" href="https://github.com/tylerachang/word-acquisition-language-models">[Code]</a></div>
				<div class="publication"><b>Chang, T. A.</b>, & Bergen, B. K. (2022). Does contextual diversity hinder early word acquisition? <i>Proceedings of the 44th Annual Conference of the Cognitive Science Society</i> (CogSci).&nbsp <a target="_blank" href="https://escholarship.org/uc/item/6r4132wr">[Paper]</a>&nbsp <a target="_blank" href="https://github.com/tylerachang/contextual-diversity">[Code]</a></div>
			</details>
			<br>
			
			<details>
				<summary style="cursor:pointer">
				<b>&nbsp Multilinguality</b>
				</summary>
				<img id="selfimage" src="research_images/language_embeddings.png" alt="Multilingual language model embeddings.">
				<div class="publication"><b>Chang, T. A.</b>, Arnett, C., Tu, Z., & Bergen, B. K. (under review). Goldfish: Monolingual language models for 350 languages.&nbsp <a target="_blank" href="https://www.arxiv.org/abs/2408.10441">[Preprint]</a>&nbsp <a target="_blank" href="https://huggingface.co/goldfish-models">[Models]</a>&nbsp <a target="_blank" href="https://github.com/tylerachang/goldfish">[Code]</a></div>
				<div class="publication"><b>Chang, T. A.</b>, Arnett, C., Tu, Z., & Bergen, B. K. (2024). When is multilinguality a curse? Language modeling for 250 high- and low-resource languages. <i>Proceedings of the Conference on Empirical Methods in Natural Language Processing</i> (EMNLP). Received an outstanding paper award.&nbsp <a target="_blank" href="https://arxiv.org/abs/2311.09205">[Preprint]</a></div>
				<div class="publication">Arnett, C.*, <b>Chang, T. A.</b>*, & Bergen, B. K. (2024). A bit of a problem: Measurement disparities in dataset sizes across languages. <i>Proceedings of the Annual Meeting of the Special Interest Group on Under-Resourced Languages</i> (workshop at LREC-COLING). *Equal contribution.&nbsp <a target="_blank" href="https://arxiv.org/abs/2403.00686">[Preprint]</a></div>
				<div class="publication"><b>Chang, T. A.</b>, Tu, Z., & Bergen, B. K. (2022). The geometry of multilingual language model representations. <i>Proceedings of the Conference on Empirical Methods in Natural Language Processing</i> (EMNLP).&nbsp <a target="_blank" href="https://aclanthology.org/2022.emnlp-main.9/">[Paper]</a>&nbsp <a target="_blank" href="https://github.com/tylerachang/multilingual-geometry">[Code]</a></div>
			</details>
			<br>
			
			<details>
				<summary style="cursor:pointer">
				<b>&nbsp Other publications</b>
				</summary>
				<br>
				<div class="publication"><b>Chang, T. A.</b>, & Bergen, B. K. (2024). Language model behavior: A comprehensive survey. <i>Computational Linguistics</i>.&nbsp <a target="_blank" href="https://arxiv.org/abs/2303.11504">[Paper]</a>&nbsp <a target="_blank" href="https://github.com/tylerachang/llm-behavior-survey">[Website]</a></div>
                <div class="publication"><b>Chang, T. A.</b>*, Tomanek, K.*, Hoffmann, J., Thain, N., van Liemt, E., Meier-Hellstern, K., & Dixon, L. (2024). Detecting hallucination and coverage errors in retrieval augmented generation for controversial topics. <i>Proceedings of the Joint International Conference on Computational Linguistics, Language Resources, and Evaluation</i> (LREC-COLING). *Equal contribution.&nbsp <a target="_blank" href="https://arxiv.org/abs/2403.08904">[Preprint]</a></div>
                <div class="publication">Shah, C.*, Chandak, Y.*, Mane, A.*, Bergen, B. K., & <b>Chang, T. A.</b> (2024). Correlations between multilingual language model geometry and crosslingual transfer performance. <i>Proceedings of the Joint International Conference on Computational Linguistics, Language Resources, and Evaluation</i> (LREC-COLING). *Undergraduate mentees.</div>
                <div class="publication">Arnett, C.*, Rivière, P. D.*, <b>Chang, T. A.</b>, & Trott, S. (2024). Different tokenization schemes lead to comparable performance in Spanish number agreement. <i>Proceedings of the SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology</i> (workshop at NAACL). *Equal contribution.&nbsp <a target="_blank" href="https://arxiv.org/abs/2403.13754">[Preprint]</a></div>
				<div class="publication">Michaelov, J. A.*, Arnett, C.*, <b>Chang, T. A.</b>, & Bergen, B. K. (2023). Structural priming demonstrates abstract grammatical representations in multilingual language models. <i>Proceedings of the Conference on Empirical Methods in Natural Language Processing</i> (EMNLP). *Equal contribution.&nbsp <a target="_blank" href="https://aclanthology.org/2023.emnlp-main.227/">[Paper]</a></div>
				<div class="publication">Arnett, C., <b>Chang, T. A.</b>, Michaelov, J., & Bergen, B. K. (2023). Crosslingual structural priming and the pre-training dynamics of bilingual language models. <i>3rd Multilingual Representation Learning Workshop</i> (workshop at EMNLP). Extended abstract.&nbsp <a target="_blank" href="https://arxiv.org/abs/2310.07929">[Abstract]</a></div>
				<div class="publication"><b>Chang, T. A.</b>, Halder, K., Anna John, N., Vyas, Y., Benajiba, Y., Ballesteros, M., & Roth, D. (2023). Characterizing and measuring linguistic dataset drift. <i>Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</i> (ACL).&nbsp <a target="_blank" href="https://aclanthology.org/2023.acl-long.498/">[Paper]</a>&nbsp <a target="_blank" href="https://github.com/amazon-science/characterizing-measuring-linguistic-drift">[Code]</a></div>
				<div class="publication">Trott, S.*, Jones, C. R.*, <b>Chang, T. A.</b>, Michaelov, J. A., & Bergen, B. K. (2023). Do large language models know what humans know? <i>Cognitive Science</i>. *Equal contribution.&nbsp <a target="_blank" href="https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.13309">[Paper]</a></div>
				<div class="publication">Jones, C. R., <b>Chang, T. A.</b>, Coulson, S., Michaelov, J. A., Trott, S., & Bergen, B. K. (2022). Distributional semantics still can’t account for affordances. <i>Proceedings of the 44th Annual Conference of the Cognitive Science Society</i> (CogSci).&nbsp <a target="_blank" href="https://escholarship.org/uc/item/44z7r3j3">[Paper]</a></div>
				<div class="publication"><b>Chang, T. A.</b>, Xu, Y., Xu, W., & Tu, Z. (2021). Convolutions and self-attention: Re-interpreting relative positions in pre-trained language models. <i>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</i> (ACL-IJCNLP).&nbsp <a target="_blank" href="https://aclanthology.org/2021.acl-long.333/">[Paper]</a>&nbsp <a target="_blank" href="https://github.com/mlpc-ucsd/BERT_Convolutions">[Code]</a></div>
				<div class="publication">Xu, W., Xu, Y., <b>Chang, T. A.</b>, & Tu, Z. (2021). Co-scale conv-attentional image transformers. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision</i> (ICCV).&nbsp <a target="_blank" href="https://arxiv.org/abs/2104.06399">[Paper]</a>&nbsp <a target="_blank" href="https://github.com/mlpc-ucsd/CoaT">[Code]</a></div>
				<div class="publication"><b>Chang, T. A.</b>, & Rafferty, A. N. (2020). Encodings of source syntax: Similarities in NMT representations across target languages. <i>Proceedings of the 5th Workshop on Representation Learning for NLP</i> (workshop at ACL).&nbsp <a target="_blank" href="https://www.aclweb.org/anthology/2020.repl4nlp-1.2/">[Link]</a></div>
				<div class="publication"><b>Chang, T. A.</b> (2020). Emergence of hierarchical syntax in neural machine translation. <i>Carleton Digital Commons, Undergraduate Thesis.</i> Carleton College, Cognitive Science Department. With distinction.&nbsp <a target="_blank" href="https://digitalcommons.carleton.edu/comps/2724/">[Link]</a>&nbsp <a target="_blank" href="paper_pdfs/Tyler_Chang-cog_sci_comps-online.pdf">[PDF]</a></div>
				<div class="publication"><b>Chang, T. A.</b> (2020). Topology of second order tensor fields. <i>Carleton Digital Commons, Undergraduate Thesis.</i> Carleton College, Mathematics Department.&nbsp <a target="_blank" href="https://digitalcommons.carleton.edu/comps/2747/">[Link]</a>&nbsp <a target="_blank" href="paper_pdfs/Tyler_Chang-math_comps-online.pdf">[PDF]</a></div>
			</details><br>
		</div>
	</div>
</body>
</html>